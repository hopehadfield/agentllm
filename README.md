# AgentLLM - Agno Provider for LiteLLM

A custom LiteLLM provider that exposes [Agno](https://github.com/agno-agi/agno) agents through an OpenAI-compatible API, enabling seamless integration with Open WebUI and other OpenAI-compatible clients.

> **Note:** This project uses LiteLLM's official `CustomLLM` extension mechanism with dynamic registration via `custom_provider_map`. No forking or monkey patching required!

## Overview

This project implements a LiteLLM custom provider for Agno agents, allowing you to:

- Expose Agno agents as OpenAI-compatible chat models
- Use Agno agents with Open WebUI or any OpenAI-compatible client
- Run agents behind a LiteLLM proxy with authentication
- Switch between different agents using model names

## Architecture

```
[Client] -> [LiteLLM Proxy :8890] -> [Agno Provider] -> [Agno Agent] -> [LLM APIs]
```

## Prerequisites

- Python 3.11+
- [uv](https://docs.astral.sh/uv/) package manager
- Google Gemini API key (get from [Google AI Studio](https://aistudio.google.com/apikey))

## Installation

1. Clone this repository:
```bash
git clone <repo-url>
cd agentllm
```

2. Install dependencies with uv:
```bash
uv sync
```

3. Configure environment variables:
```bash
cp .env.example .env
# Edit .env and add your GEMINI_API_KEY
```

## Project Structure

```
agentllm/
├── src/agentllm/
│   ├── provider/
│   │   └── transformation.py    # LiteLLM provider implementation
│   ├── agents/
│   │   └── examples.py          # Example Agno agents
│   └── proxy_config.yaml        # LiteLLM proxy configuration
├── tests/
│   └── test_provider.py         # TDD unit tests
├── noxfile.py                   # Task automation
└── pyproject.toml               # Project configuration
```

## Usage

### Running Tests

```bash
# Run unit tests
nox -s test

# Run integration tests
nox -s integration

# Run specific test
uv run pytest tests/test_custom_handler.py::TestAgnoCustomLLM -v
```

### Starting the Proxy

```bash
# Start LiteLLM proxy with Agno provider
nox -s proxy

# Or manually:
uv run litellm --config src/agentllm/proxy_config.yaml --port 8890
```

### Making Requests

Once the proxy is running, you can make OpenAI-compatible requests:

```bash
curl -X POST http://localhost:8890/v1/chat/completions \
  -H "Authorization: Bearer sk-agno-test-key-12345" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "agno/assistant",
    "messages": [
      {"role": "user", "content": "Hello!"}
    ]
  }'
```

### Available Models

**Agno Agents** (powered by Gemini 2.0 Flash):
- `agno/echo` - Simple echo agent for testing
- `agno/assistant` - General-purpose helpful assistant
- `agno/code-helper` - Coding assistant for programming tasks

**Direct Gemini Models:**
- `gemini-2.5-flash` - Latest, fastest Gemini model
- `gemini-2.0-flash` - Stable, efficient model (used by Agno agents)
- `gemini-1.5-pro` - Most capable Gemini model
- `gemini-1.5-flash` - Fast and efficient

> **Note:** All models require a single `GEMINI_API_KEY` in your `.env` file. Get your API key from [Google AI Studio](https://aistudio.google.com/apikey).

## How It Works

This project uses LiteLLM's **official CustomLLM extension mechanism** - no forking or monkey patching required!

### Dynamic Registration

The Agno provider is registered via `custom_provider_map` in `proxy_config.yaml`:

```yaml
litellm_settings:
  custom_provider_map:
    - provider: "agno"
      custom_handler: custom_handler.agno_handler
```

This is the **recommended approach** from LiteLLM for adding custom providers without modifying the LiteLLM codebase.

### Implementation

The provider extends `litellm.CustomLLM` base class and implements:
- `completion()` - Synchronous completions
- `streaming()` - Streaming responses
- `acompletion()` - Async completions (future enhancement)

See [LiteLLM CustomLLM Docs](https://docs.litellm.ai/docs/providers/custom_llm_server) for details.

## Development Workflow

This project follows Test-Driven Development (TDD):

1. Write a failing test
2. Implement the feature
3. Run tests: `nox -s test`
4. Refactor if needed

### Adding New Agents

1. Define your agent in `src/agentllm/agents/examples.py`:

```python
def create_my_agent() -> Agent:
    return Agent(
        name="my-agent",
        description="My custom agent",
        instructions=["Your instructions here"],
        markdown=True,
    )

# Register in AGENT_REGISTRY
AGENT_REGISTRY["my-agent"] = create_my_agent
```

2. Add to `proxy_config.yaml`:

```yaml
  - model_name: agno/my-agent
    litellm_params:
      model: agno/my-agent
      api_base: http://localhost/agno
      custom_llm_provider: agno
```

3. Restart the proxy

## Running with Open WebUI

### Quick Start

**Prerequisites**: Make sure you have your Gemini API key configured (all agents and models use Gemini):
```bash
export GEMINI_API_KEY="your-gemini-api-key-here"
```

1. **Start the LiteLLM proxy** (in one terminal):
   ```bash
   nox -s proxy
   ```

2. **Start Open WebUI** (in another terminal):
   ```bash
   docker compose up
   ```

3. **Access Open WebUI**:
   - Open your browser to http://localhost:3000
   - Create an account (first user becomes admin)
   - The Agno models will be automatically available!

4. **Select an Agno agent**:
   - Click on the model selector
   - Choose `agno/assistant`, `agno/echo`, or `agno/code-helper`
   - Start chatting with your Agno agents!

### Configuration

The `compose.yaml` connects Open WebUI to your LiteLLM proxy with:
- **API Base**: `http://host.docker.internal:8890/v1`
- **API Key**: `sk-agno-test-key-12345`
- **Web UI**: http://localhost:3000

### Stopping

```bash
# Stop Open WebUI
docker compose down

# Stop proxy
# Press Ctrl+C in the terminal running nox -s proxy
```

## Configuration

### Environment Variables

See `.env.example` for all configuration options. Key variables include:

- **GEMINI_API_KEY** - Required for all models (Agno agents and direct Gemini models). Get from [Google AI Studio](https://aistudio.google.com/apikey)
- **LITELLM_MASTER_KEY** - API key for accessing the LiteLLM proxy (default: `sk-agno-test-key-12345`)

### Proxy Configuration

Edit `src/agentllm/proxy_config.yaml` to:
- Add/remove agent models (Agno agents, Gemini, or other LLM providers)
- Change authentication settings
- Configure logging
- Adjust server settings

The proxy already includes configurations for:
- **Agno agents** (custom agents with specialized behaviors)
- **Google Gemini models** (gemini-2.5-flash, gemini-2.0-flash, gemini-1.5-pro, gemini-1.5-flash)

## Provider Implementation

The Agno provider extends `litellm.CustomLLM` and implements:

- `completion()` - Synchronous completions with full agent execution
- `streaming()` - Synchronous streaming returning GenericStreamingChunk format
- `acompletion()` - Async completions using `agent.arun()`
- `astreaming()` - **True real-time streaming** using `agent.arun(stream=True)` ⚡
- Dynamic registration via `custom_provider_map` in config
- Parameter pass-through - `temperature` and `max_tokens` are passed to agent's model
- Conversation context - Previous messages are preserved in agent's session

### Streaming Support

LiteLLM's `CustomLLM` requires streaming methods to return `GenericStreamingChunk` dictionaries, **not** `ModelResponse` objects. The key format requirements:

**GenericStreamingChunk Format:**
```python
{
    "text": "content here",           # Use "text", not "content" or "delta"
    "finish_reason": "stop" or None,
    "index": 0,
    "is_finished": True or False,
    "tool_use": None,
    "usage": {"completion_tokens": 0, "prompt_tokens": 0, "total_tokens": 0}
}
```

**Sync streaming** (`streaming()`): Gets the complete response from `completion()` and returns it as a single GenericStreamingChunk.

**Async streaming** (`astreaming()`): Uses Agno's native `async for` streaming with `agent.arun(stream=True)` for true real-time token-by-token streaming, yielding each chunk in GenericStreamingChunk format.

**Common Pitfall:** Do NOT return `ModelResponse` objects from streaming methods - LiteLLM's streaming handler expects the `GenericStreamingChunk` dictionary format with a `text` field. Returning `ModelResponse` will cause `AttributeError: 'ModelResponse' object has no attribute 'text'`.

This approach requires **no modifications to LiteLLM** - it's a pure plugin using official extension APIs.

## Troubleshooting

### Tests Fail with "No module named 'agentllm'"

```bash
uv pip install -e .
```

### Agent Fails to Initialize

Ensure you have set `GEMINI_API_KEY` in your `.env` file. Get your key from [Google AI Studio](https://aistudio.google.com/apikey).

### Proxy Won't Start

Check that port 8890 is available:
```bash
lsof -i :8890
```

## Contributing

1. Write tests for new features
2. Follow TDD workflow
3. Run `nox -s lint` and `nox -s format`
4. Update documentation

## License

[Your License Here]

## References

- [Agno Framework](https://github.com/agno-agi/agno)
- [LiteLLM](https://github.com/BerriAI/litellm)
- [LiteLLM Provider Registration](https://docs.litellm.ai/docs/provider_registration/)
- [Open WebUI](https://github.com/open-webui/open-webui)
