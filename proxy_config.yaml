# LiteLLM Proxy Configuration for Agno Provider
# This configuration exposes Agno agents as OpenAI-compatible models

model_list:
  - model_name: agno/release-manager
    litellm_params:
      model: agno/release-manager
      custom_llm_provider: agno
    model_info:
      description: "Release management assistant for software releases, changelogs, and version planning"
      mode: chat

  # Google Gemini 2.5 models
  - model_name: gemini-2.5-pro
    litellm_params:
      model: gemini/gemini-2.5-pro
      api_key: os.environ/GEMINI_API_KEY
    model_info:
      description: "Google Gemini 2.5 Pro - Most capable model"
      mode: chat

  - model_name: gemini-2.5-flash
    litellm_params:
      model: gemini/gemini-2.5-flash
      api_key: os.environ/GEMINI_API_KEY
    model_info:
      description: "Google Gemini 2.5 Flash - Fast and efficient"
      mode: chat

# General configuration
general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY

  # Disable authentication for health checks (Kubernetes probes)
  disable_health_check_auth: true

  # Map OpenWebUI headers to LiteLLM user tracking
  # Enable ENABLE_FORWARD_USER_INFO_HEADERS=true in OpenWebUI
  user_header_name: X-OpenWebUI-User-Id
  user_header_mappings:
    - header_name: X-OpenWebUI-User-Id
      litellm_user_role: internal_user
    - header_name: X-OpenWebUI-User-Email
      litellm_user_role: customer

litellm_settings:
  drop_params: true  # Drop unsupported params instead of erroring
  set_verbose: false  # Disable verbose logging
  json_logs: true    # Enable JSON logs for better log aggregation
  success_callback: []
  failure_callback: []
  # Register custom Agno provider handler
  custom_provider_map:
    - provider: "agno"
      custom_handler: custom_handler.agno_handler

# Server settings
server:
  host: 0.0.0.0
  port: 8890
