services:
  litellm-proxy:
    build:
      context: .
      dockerfile: Containerfile
    container_name: litellm-proxy
    ports:
      - "8890:8890"
    # Load environment variables from shared files
    env_file:
      - .env.shared          # Non-sensitive shared configuration
      - .env.secrets         # Sensitive credentials (git-ignored)
    volumes:
      # Mount local code for development (hot reload)
      - ./src/agentllm:/app/agentllm
      # Persist database and workspace
      - litellm-data:/app/tmp
      # Mount config from project root (for LiteLLM path resolution)
      - ./proxy_config.yaml:/app/proxy_config.yaml:ro
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8890/health/readiness"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    ports:
      - "3000:8080"
    # Load environment variables from shared files
    env_file:
      - .env.shared          # Non-sensitive shared configuration
      - .env.secrets         # Sensitive credentials (git-ignored)
    environment:
      # Override: Point to LiteLLM proxy (note: different variable name for OpenWebUI)
      # Compose uses OPENAI_API_BASE_URLS (plural) with the value from OPENAI_API_BASE_URL (singular)
      # Dev mode (local proxy): http://host.docker.internal:8890/v1
      # Production mode (both containerized): http://litellm-proxy:8890/v1
      OPENAI_API_BASE_URLS: "${OPENAI_API_BASE_URL:-http://litellm-proxy:8890/v1}"
      OPENAI_API_KEYS: "${LITELLM_MASTER_KEY:-sk-agno-test-key-12345}"

    # Enable host.docker.internal on all platforms (including Linux)
    extra_hosts:
      - "host.docker.internal:host-gateway"

    volumes:
      - open-webui:/app/backend/data
    # depends_on:
    #   litellm-proxy:
    #     condition: service_healthy
    restart: unless-stopped

volumes:
  open-webui:
  litellm-data:
