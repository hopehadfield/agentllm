"""Custom LiteLLM handler for Agno provider using dynamic registration."""

import os
import sys
from collections.abc import AsyncIterator, Iterator
from pathlib import Path
from typing import Any

import litellm
from litellm import CustomLLM
from litellm.types.utils import Choices, Message, ModelResponse
from loguru import logger

from agentllm.agents.release_manager import ReleaseManager

# Configure logging for our custom handler using loguru
# Remove default handler
logger.remove()

# Determine log file path - use temp directory (consistent with DB and gdrive workspace)
log_dir = os.getenv("AGENTLLM_DATA_DIR", "tmp")
log_file = Path(log_dir) / "agno_handler.log"

# Add file handler for detailed logs (DEBUG level)
logger.add(
    log_file,
    level="DEBUG",
    format="{time:YYYY-MM-DD HH:mm:ss.SSS} | {level: <8} | {name}:{function}:{line} - {message}",
    rotation="10 MB",
    retention="7 days",
)

# Add console handler for important logs only (INFO level)
logger.add(
    sys.stderr,
    level="INFO",
    format="<green>{time:YYYY-MM-DD HH:mm:ss.SSS}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan> - <level>{message}</level>",
)


class AgnoCustomLLM(CustomLLM):
    """Custom LiteLLM handler for Agno agents.

    This allows dynamic registration without modifying LiteLLM source code.
    Supports Agno session management for conversation continuity.
    """

    def __init__(self):
        """Initialize the custom LLM handler with agent cache."""
        super().__init__()
        # Cache agents by (agent_name, temperature, max_tokens, user_id)
        self._agent_cache: dict[tuple, Any] = {}
        logger.info("Initialized AgnoCustomLLM with agent caching")

    def _extract_session_info(self, kwargs: dict[str, Any]) -> tuple[str | None, str | None]:
        """Extract session_id and user_id from request kwargs.

        Checks multiple sources in priority order:
        1. Request body metadata (from OpenWebUI pipe functions)
        2. OpenWebUI headers (X-OpenWebUI-User-Id, X-OpenWebUI-Chat-Id)
        3. LiteLLM metadata
        4. User field

        Args:
            kwargs: Request parameters

        Returns:
            Tuple of (session_id, user_id)
        """
        logger.debug("_extract_session_info() called")
        session_id = None
        user_id = None

        # 1. Check request body for metadata (from OpenWebUI pipe functions)
        litellm_params = kwargs.get("litellm_params", {})
        proxy_request = litellm_params.get("proxy_server_request", {})
        request_body = proxy_request.get("body", {})
        body_metadata = request_body.get("metadata", {})

        if body_metadata:
            session_id = body_metadata.get("session_id") or body_metadata.get("chat_id")
            user_id = body_metadata.get("user_id")
            logger.info(f"[1/4] Found in body metadata: session_id={session_id}, user_id={user_id}")

        # 2. Check OpenWebUI headers (ENABLE_FORWARD_USER_INFO_HEADERS)
        headers = litellm_params.get("metadata", {}).get("headers", {})
        if not session_id and headers:
            # Check for chat_id header (might be X-OpenWebUI-Chat-Id)
            session_id = headers.get("x-openwebui-chat-id") or headers.get("X-OpenWebUI-Chat-Id")
            if session_id:
                logger.info(f"[2/4] Found in headers: session_id={session_id}")

        if not user_id and headers:
            # Check for user_id header
            user_id = (
                headers.get("x-openwebui-user-id")
                or headers.get("X-OpenWebUI-User-Id")
                or headers.get("x-openwebui-user-email")
                or headers.get("X-OpenWebUI-User-Email")
            )
            if user_id:
                logger.info(f"[2/4] Found in headers: user_id={user_id}")

        # 3. Check LiteLLM metadata
        if not session_id and "litellm_params" in kwargs:
            litellm_metadata = litellm_params.get("metadata", {})
            session_id = litellm_metadata.get("session_id") or litellm_metadata.get(
                "conversation_id"
            )
            if session_id:
                logger.info(f"[3/4] Found in LiteLLM metadata: session_id={session_id}")

        # 4. Fallback to user field
        if not user_id:
            user_id = kwargs.get("user")
            if user_id:
                logger.info(f"[4/4] Found in user field: user_id={user_id}")

        # Log what we're using
        logger.info(f"âœ“ Final extracted session info: user_id={user_id}, session_id={session_id}")

        # Log full structure for debugging (only if nothing found)
        if not session_id and not user_id:
            logger.warning("âš  No session/user info found! Logging full request structure:")
            logger.warning(f"Headers available: {list(headers.keys()) if headers else 'None'}")
            logger.warning(
                f"Body metadata keys: {list(body_metadata.keys()) if body_metadata else 'None'}"
            )
            logger.warning(
                f"LiteLLM metadata keys: {list(litellm_params.get('metadata', {}).keys())}"
            )

        return session_id, user_id

    def _get_agent(self, model: str, user_id: str | None = None, **kwargs):
        """Get agent instance from model name with parameters.

        Uses caching to reuse agent instances for the same configuration and user.

        Args:
            model: Model name (e.g., "agno/release-manager" or just "release-manager")
            user_id: User ID for agent isolation
            **kwargs: Additional parameters (temperature, max_tokens, etc.)

        Returns:
            Agent instance (cached or newly created)

        Raises:
            Exception: If agent not found
        """
        logger.debug(f"_get_agent() called with model={model}, user_id={user_id}")

        # Extract agent name from model (handle both "agno/release-manager" and "release-manager")
        agent_name = model.replace("agno/", "")
        logger.debug(f"Extracted agent_name: {agent_name}")

        # Extract OpenAI parameters to pass to agent
        temperature = kwargs.get("temperature")
        max_tokens = kwargs.get("max_tokens")
        logger.debug(f"Agent parameters: temperature={temperature}, max_tokens={max_tokens}")

        # Build cache key from agent configuration and user_id
        cache_key = (agent_name, temperature, max_tokens, user_id)

        # Check if agent exists in cache
        if cache_key in self._agent_cache:
            logger.info(f"âœ“ Using CACHED agent for key: {cache_key}")
            return self._agent_cache[cache_key]

        # Create new agent and cache it
        logger.info(f"âœ— Cache MISS - Creating NEW agent for key: {cache_key}")

        # Instantiate the agent class based on agent_name
        if agent_name == "release-manager":
            logger.debug("Instantiating ReleaseManager...")
            agent = ReleaseManager(temperature=temperature, max_tokens=max_tokens)
            logger.debug("ReleaseManager instantiated successfully")
        else:
            error_msg = f"Agent '{agent_name}' not found. Only 'release-manager' is available."
            logger.error(error_msg)
            raise Exception(error_msg)

        self._agent_cache[cache_key] = agent
        logger.info(f"âœ“ Agent cached. Total cached agents: {len(self._agent_cache)}")
        logger.debug(f"Cache keys: {list(self._agent_cache.keys())}")
        return agent

    def _build_response(self, model: str, content: str) -> ModelResponse:
        """Build a ModelResponse from agent output.

        Args:
            model: Model name
            content: Agent response content

        Returns:
            ModelResponse object
        """
        logger.debug(f"_build_response() called for model={model}, content_length={len(content)}")
        message = Message(role="assistant", content=content)
        choice = Choices(finish_reason="stop", index=0, message=message)

        model_response = ModelResponse()
        model_response.model = model
        model_response.choices = [choice]
        model_response.usage = {
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0,
        }

        logger.debug("_build_response() completed successfully")
        return model_response

    def _extract_request_params(
        self, messages: list[dict[str, Any]], kwargs: dict[str, Any]
    ) -> tuple[str, str | None, str | None]:
        """Extract common request parameters.

        Args:
            messages: OpenAI-format messages
            kwargs: Request parameters

        Returns:
            Tuple of (user_message, session_id, user_id)
        """
        logger.debug("_extract_request_params() called")
        user_message = self._extract_user_message(messages)
        logger.debug(f"Extracted user_message (length={len(user_message)})")
        session_id, user_id = self._extract_session_info(kwargs)
        logger.debug(f"Extracted session_id={session_id}, user_id={user_id}")
        return user_message, session_id, user_id

    def completion(
        self,
        model: str,
        messages: list[dict[str, Any]],
        api_base: str | None = None,
        custom_llm_provider: str = "agno",
        **kwargs,
    ) -> ModelResponse:
        """Handle completion requests for Agno agents.

        Args:
            model: Model name (e.g., "agno/release-manager" or just "release-manager")
            messages: OpenAI-format messages
            api_base: API base URL (not used for in-process)
            custom_llm_provider: Provider name
            **kwargs: Additional parameters (stream, temperature, etc.)

        Returns:
            ModelResponse object
        """
        logger.info("=" * 80)
        logger.info(f">>> completion() STARTED - model={model}")
        logger.info(f"kwargs: {kwargs}")
        logger.info(f"messages: {messages}")

        # Check if streaming is requested
        stream = kwargs.get("stream", False)
        if stream:
            logger.info("Streaming requested, delegating to streaming()")
            # Return streaming iterator
            return self.streaming(
                model=model,
                messages=messages,
                api_base=api_base,
                custom_llm_provider=custom_llm_provider,
                **kwargs,
            )

        logger.info("Extracting request parameters...")
        # Extract request parameters first (need user_id for agent cache)
        user_message, session_id, user_id = self._extract_request_params(messages, kwargs)
        logger.info(
            f"Extracted: user_message_length={len(user_message)}, session_id={session_id}, user_id={user_id}"
        )

        logger.info("Getting agent instance...")
        # Get agent instance (with caching based on user_id)
        agent = self._get_agent(model, user_id=user_id, **kwargs)

        logger.info(f"Running agent with session_id={session_id}, user_id={user_id}")
        # Run the agent with session management
        response = agent.run(user_message, stream=False, session_id=session_id, user_id=user_id)
        logger.info(f"Agent run completed, response type: {type(response)}")

        # Extract content and build response
        content = response.content if hasattr(response, "content") else str(response)
        logger.info(f"Extracted content length: {len(content)}")

        result = self._build_response(model, str(content))
        logger.info(f"<<< completion() FINISHED - model={model}")
        logger.info("=" * 80)
        return result

    def streaming(
        self,
        model: str,
        messages: list[dict[str, Any]],
        api_base: str | None = None,
        custom_llm_provider: str = "agno",
        **kwargs,
    ) -> Iterator[dict[str, Any]]:
        """Handle streaming requests for Agno agents.

        Note: Streaming is not fully supported in sync mode.
        Returns a single complete response instead of chunks.
        For true streaming, use async requests which will call astreaming().

        Args:
            model: Model name
            messages: OpenAI-format messages
            api_base: API base URL (not used)
            custom_llm_provider: Provider name
            **kwargs: Additional parameters

        Yields:
            GenericStreamingChunk dictionary with text field
        """
        logger.info("=" * 80)
        logger.info(f">>> streaming() STARTED - model={model}")
        logger.info(f"kwargs: {kwargs}")

        logger.info(
            "Getting complete response via completion() (sync streaming not fully supported)"
        )
        # Get the complete response
        result = self.completion(
            model=model,
            messages=messages,
            api_base=api_base,
            custom_llm_provider=custom_llm_provider,
            **{k: v for k, v in kwargs.items() if k != "stream"},
        )

        # Extract content from the ModelResponse
        content = ""
        if result.choices and len(result.choices) > 0:
            content = result.choices[0].message.content or ""

        logger.info(f"Yielding single streaming chunk with content_length={len(content)}")
        # Return as GenericStreamingChunk format (required by CustomLLM interface)
        chunk = {
            "text": content,
            "finish_reason": "stop",
            "index": 0,
            "is_finished": True,
            "tool_use": None,
            "usage": {
                "completion_tokens": (
                    result.usage.get("completion_tokens", 0) if result.usage else 0
                ),
                "prompt_tokens": (result.usage.get("prompt_tokens", 0) if result.usage else 0),
                "total_tokens": (result.usage.get("total_tokens", 0) if result.usage else 0),
            },
        }
        yield chunk
        logger.info(f"<<< streaming() FINISHED - model={model}")
        logger.info("=" * 80)

    async def acompletion(
        self,
        model: str,
        messages: list[dict[str, Any]],
        api_base: str | None = None,
        custom_llm_provider: str = "agno",
        **kwargs,
    ) -> ModelResponse:
        """Async completion using agent.arun().

        Args:
            model: Model name (e.g., "agno/release-manager" or just "release-manager")
            messages: OpenAI-format messages
            api_base: API base URL (not used for in-process)
            custom_llm_provider: Provider name
            **kwargs: Additional parameters (stream, temperature, etc.)

        Returns:
            ModelResponse object
        """
        logger.info("=" * 80)
        logger.info(f">>> acompletion() STARTED - model={model}")
        logger.info(f"kwargs: {kwargs}")
        logger.info(f"messages: {messages}")

        logger.info("Extracting request parameters...")
        # Extract request parameters first (need user_id for agent cache)
        user_message, session_id, user_id = self._extract_request_params(messages, kwargs)
        logger.info(
            f"Extracted: user_message_length={len(user_message)}, session_id={session_id}, user_id={user_id}"
        )

        logger.info("Getting agent instance...")
        # Get agent instance (with caching based on user_id)
        agent = self._get_agent(model, user_id=user_id, **kwargs)

        logger.info(f"Running agent asynchronously with session_id={session_id}, user_id={user_id}")
        # Run the agent asynchronously with session management
        response = await agent.arun(
            user_message, stream=False, session_id=session_id, user_id=user_id
        )
        logger.info(f"Agent arun completed, response type: {type(response)}")

        # Extract content and build response
        content = response.content if hasattr(response, "content") else str(response)
        logger.info(f"Extracted content length: {len(content)}")

        result = self._build_response(model, str(content))
        logger.info(f"<<< acompletion() FINISHED - model={model}")
        logger.info("=" * 80)
        return result

    async def astreaming(
        self,
        model: str,
        messages: list[dict[str, Any]],
        api_base: str | None = None,
        custom_llm_provider: str = "agno",
        **kwargs,
    ) -> AsyncIterator[dict[str, Any]]:
        """Async streaming using Agno's native streaming support.

        Args:
            model: Model name
            messages: OpenAI-format messages
            api_base: API base URL (not used)
            custom_llm_provider: Provider name
            **kwargs: Additional parameters

        Yields:
            GenericStreamingChunk dictionaries with text field
        """
        logger.info("=" * 80)
        logger.info(f">>> astreaming() STARTED - model={model}")
        logger.info(f"kwargs: {kwargs}")
        logger.info(f"messages: {messages}")

        logger.info("Extracting request parameters...")
        # Extract request parameters first (need user_id for agent cache)
        user_message, session_id, user_id = self._extract_request_params(messages, kwargs)
        logger.info(
            f"Extracted: user_message_length={len(user_message)}, session_id={session_id}, user_id={user_id}"
        )

        logger.info("Getting agent instance...")
        # Get agent instance (with caching based on user_id)
        agent = self._get_agent(model, user_id=user_id, **kwargs)

        logger.info(f"Starting async streaming with session_id={session_id}, user_id={user_id}")
        # Use Agno's real async streaming with session management
        chunk_count = 0

        logger.info("Calling agent.arun() with stream=True...")
        stream = agent.arun(user_message, stream=True, session_id=session_id, user_id=user_id)
        logger.debug(f"agent.arun() returned stream type: {type(stream)}")

        logger.info("Entering async for loop over ReleaseManager stream...")
        async for chunk in stream:
            chunk_count += 1
            chunk_type = type(chunk).__name__

            # Extract content from chunk
            content = chunk.content if hasattr(chunk, "content") else str(chunk)

            logger.debug(
                f"[custom_handler] Received chunk #{chunk_count} from ReleaseManager: "
                f"type={chunk_type}, content_length={len(content)}, "
                f"has_content={bool(content)}"
            )

            if not content:
                logger.debug(f"Skipping empty chunk #{chunk_count}")
                continue

            logger.debug(f"Yielding chunk #{chunk_count} to LiteLLM, content_length={len(content)}")
            # Yield GenericStreamingChunk format
            yield {
                "text": content,
                "finish_reason": None,
                "index": 0,
                "is_finished": False,
                "tool_use": None,
                "usage": {
                    "completion_tokens": 0,
                    "prompt_tokens": 0,
                    "total_tokens": 0,
                },
            }
            logger.debug(f"Chunk #{chunk_count} yielded to LiteLLM successfully")

        logger.info(
            f"async for loop over ReleaseManager stream completed, total chunks: {chunk_count}"
        )
        logger.info("Sending final chunk with finish_reason=stop")
        # Send final chunk with finish_reason
        yield {
            "text": "",
            "finish_reason": "stop",
            "index": 0,
            "is_finished": True,
            "tool_use": None,
            "usage": {
                "completion_tokens": chunk_count,
                "prompt_tokens": 0,
                "total_tokens": chunk_count,
            },
        }
        logger.info(f"<<< astreaming() FINISHED - model={model}")
        logger.info("=" * 80)

    def _extract_user_message(self, messages: list[dict[str, Any]]) -> str:
        """Extract the last user message from messages list.

        Args:
            messages: OpenAI-format messages

        Returns:
            User message content
        """
        logger.debug(f"_extract_user_message() called with {len(messages)} messages")

        # Find the last user message
        for idx, message in enumerate(reversed(messages)):
            if message.get("role") == "user":
                content = message.get("content", "")
                logger.debug(
                    f"Found user message at position {len(messages) - idx - 1} (length={len(content)})"
                )
                return content

        # If no user message found, concatenate all messages
        logger.warning("No user message found, concatenating all messages")
        combined = " ".join(msg.get("content", "") for msg in messages)
        logger.debug(f"Combined message length: {len(combined)}")
        return combined

    # Note: _add_messages_to_agent() method removed
    # Agno now handles conversation history automatically via:
    # - db=shared_db (enables session storage)
    # - add_history_to_context=True (adds previous messages to context)
    # - session_id/user_id passed to agent.run()


# Create a singleton instance
agno_handler = AgnoCustomLLM()


# Register the handler
def register_agno_provider():
    """Register the Agno provider with LiteLLM.

    Call this before using the proxy or making completion calls.
    """
    litellm.custom_provider_map = [{"provider": "agno", "custom_handler": agno_handler}]
    print("âœ… Registered Agno provider with LiteLLM")


if __name__ == "__main__":
    # Auto-register when run as script
    register_agno_provider()
    print("\nðŸš€ Agno provider registered!")
    print("   You can now use models like: agno/release-manager")
